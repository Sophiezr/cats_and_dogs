{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a0940b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sn\n",
    "from torchvision import ops, transforms as trans\n",
    "import torchvision\n",
    "from torchvision.transforms import functional as F\n",
    "import warnings\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329961fd",
   "metadata": {},
   "source": [
    "My proposed solution for this dataset has two parts:\n",
    "- Find two-pet images using Torchvision *fasterrcnn_resnet50_fpn* and split them into two seperate images since we have images with two pets but don't have location information. We find if we have a pet or two pets in the images.\n",
    "- Build a multi-headed classification and segmentation model that outputs breed, cat or dog (since the breed is unique), and the mask.\n",
    "\n",
    "In this notbook, I calculate/present performance metrics for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0547647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path = 'inputs/csv/df_test.csv'\n",
    "# Please update the directory based on the data directory path on your local machine.\n",
    "img_dir = '/media/sophie/SDD_Data/Data/cats_and_dogs/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe42c6d",
   "metadata": {},
   "source": [
    "### Pet detection model performance\n",
    "Torchvision *fasterrcnn_resnet50_fpn* object detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cf53fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area(bbox):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    width = x2 - x1\n",
    "    length = y2 - y1\n",
    "    return width * length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "257a28c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_two_biggest_bbox(bbox):\n",
    "    # Calculate areas of bounding boxes\n",
    "    areas = [(box[2] - box[0]) * (box[3] - box[1]) for box in bbox]\n",
    "\n",
    "    # Sort the bounding boxes based on area (descending order)\n",
    "    sorted_boxes = sorted(zip(bbox, areas), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the two biggest bounding boxes\n",
    "    big_box_0 = sorted_boxes[0][0]\n",
    "    big_box_1 = sorted_boxes[1][0]\n",
    "    bboxes = [big_box_0, big_box_1]\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbb45816",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(box1, box2):\n",
    "    # Extract the coordinates of the boxes\n",
    "    x1_box1, y1_box1, x2_box1, y2_box1 = box1\n",
    "    x1_box2, y1_box2, x2_box2, y2_box2 = box2\n",
    "\n",
    "    # Calculate the coordinates of the intersection rectangle\n",
    "    x_left = max(x1_box1, x1_box2)\n",
    "    y_top = max(y1_box1, y1_box2)\n",
    "    x_right = min(x2_box1, x2_box2)\n",
    "    y_bottom = min(y2_box1, y2_box2)\n",
    "\n",
    "    # Calculate the area of intersection rectangle\n",
    "    intersection_area = max(0, x_right - x_left + 1) * max(0, y_bottom - y_top + 1)\n",
    "\n",
    "    # Calculate the area of both bounding boxes\n",
    "    box1_area = (x2_box1 - x1_box1 + 1) * (y2_box1 - y1_box1 + 1)\n",
    "    box2_area = (x2_box2 - x1_box2 + 1) * (y2_box2 - y1_box2 + 1)\n",
    "\n",
    "    # Calculate the union area by subtracting the intersection area\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    # Calculate the IoU\n",
    "    iou = intersection_area / union_area\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5045567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_of_pets(bbox):\n",
    "    \"\"\"\n",
    "    Set of rules to detect number of pets in an image.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(bbox) > 2:\n",
    "        # find two biggest bounding boxes if more than two objects are detected.\n",
    "        bbox = find_two_biggest_bbox(bbox)\n",
    "    if len(bbox) == 0:\n",
    "        # we know there is at least one pet in the image even if it is not detected\n",
    "        num_of_pets = 1\n",
    "    elif len(bbox) == 1:\n",
    "        num_of_pets = 1\n",
    "    else:\n",
    "        area_0 = calculate_area(bbox[0])\n",
    "        area_1 = calculate_area(bbox[1])\n",
    "        IoU = calculate_iou(bbox[0], bbox[1])\n",
    "        if IoU > 0.5:\n",
    "            num_of_pets = 1\n",
    "        # if one bbox is much smaller than the other, it is a random bbox.\n",
    "        elif area_1 < area_0 / 2 or area_0 < area_1 / 2:\n",
    "            num_of_pets = 1\n",
    "        else:\n",
    "            num_of_pets = 2\n",
    "\n",
    "    return num_of_pets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd974a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_num_of_pets(model, img):\n",
    "    image_tensor = F.to_tensor(img)\n",
    "    image_tensor = torch.unsqueeze(image_tensor, 0)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "    # Process the predictions\n",
    "    boxes = predictions[0]['boxes']\n",
    "    scores = predictions[0]['scores']\n",
    "\n",
    "    # Filter predictions\n",
    "    cat_dog_indices = [i for i, score in enumerate(scores) if score > 0.85]\n",
    "    filtered_boxes = boxes[cat_dog_indices]\n",
    "    pets = get_num_of_pets(filtered_boxes.numpy())\n",
    "    return pets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dfeb625",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2f98efa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pet_id</th>\n",
       "      <th>label</th>\n",
       "      <th>breed_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2202c98d-3950-5138-9724-0175c63ba4c6</td>\n",
       "      <td>[634]</td>\n",
       "      <td>0</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8eac65ff-3695-56d4-b233-e8fd7f3275f8</td>\n",
       "      <td>[3455]</td>\n",
       "      <td>0</td>\n",
       "      <td>[5]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3f1eaf17-25cb-5a77-8b38-c08ff7bf2efb</td>\n",
       "      <td>[4597]</td>\n",
       "      <td>1</td>\n",
       "      <td>[16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84982479-4c5c-5c2b-98b3-e09c7c6fa79f</td>\n",
       "      <td>[6232]</td>\n",
       "      <td>1</td>\n",
       "      <td>[14]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4f8a373d-026f-5bed-b9b0-404e9485d281</td>\n",
       "      <td>[210]</td>\n",
       "      <td>1</td>\n",
       "      <td>[21]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  pet_id  label breed_label\n",
       "0  2202c98d-3950-5138-9724-0175c63ba4c6   [634]      0         [5]\n",
       "1  8eac65ff-3695-56d4-b233-e8fd7f3275f8  [3455]      0         [5]\n",
       "2  3f1eaf17-25cb-5a77-8b38-c08ff7bf2efb  [4597]      1        [16]\n",
       "3  84982479-4c5c-5c2b-98b3-e09c7c6fa79f  [6232]      1        [14]\n",
       "4  4f8a373d-026f-5bed-b9b0-404e9485d281   [210]      1        [21]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3670114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load torchvision object detection model to detect number of pets in the image.\n",
    "od_model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "correct = 0\n",
    "wrong = 0\n",
    "od_model.eval()\n",
    "for i, row in df_test.iterrows():\n",
    "    \n",
    "    image = Image.open(img_dir + row['id'] + '/image.jpg')\n",
    "    n_pets = detect_num_of_pets(od_model, image)\n",
    "    \n",
    "    breed = np.array(literal_eval(row['breed_label']))\n",
    "    if n_pets == len(breed):\n",
    "        correct += 1\n",
    "    \n",
    "    else:\n",
    "        wrong += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b43a9db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9065934065934066"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy\n",
    "accuracy = correct/len(df_test) \n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35430dc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_pet_images = df_test['breed_label'].apply(lambda x: len(np.array(literal_eval(x))) > 1).sum()\n",
    "two_pet_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07454eb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_pet_images = len(df_test) - two_pet_images\n",
    "single_pet_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3416363a",
   "metadata": {},
   "source": [
    "The object detection model achieves an accuracy rate of approximately 90%. While this metric provides some indication of performance, it may not be the most suitable measure due to a class imbalance between single-pet and two-pet images. To gain better insights in real-world scenarios, alternative metrics should be considered. Nevertheless, when considering the images themselves, the accuracy metric does reflect the actual performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f39bf9",
   "metadata": {},
   "source": [
    "### Multi-headed segmentation classification model\n",
    "I built a UNet architecture for both segmentation and classification tasks (pet mask and breed detection). The model was trained for 15 epochs, and the performance of the classification and segmentation heads was evaluated. It should be noted that due to time limitations, the model could not be trained for an extended period, resulting in unsatisfactory performance.\n",
    "All metric calculations are done in *test.py* and a comprehensive presentation of the metrics can be found in the *outout/metrics*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df134f1a",
   "metadata": {},
   "source": [
    "I used Mean Dice Coefficient metric to calculate the performance of the segmentation head. The Mean Dice Coefficient is *0.76269*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea3623",
   "metadata": {},
   "source": [
    "To evaluate the performance of the breed classification task, I utilize the classification performance report provided by the *sklearn* library. These methods allow for a detailed analysis of the model's performance, including metrics such as precision, recall, F1 score, and accuracy. The following is overall weighted average precision, recall, and F1-score. Comprehensive presentation of the metrics and confusion matrix can be found in the outout/metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92622fdf",
   "metadata": {},
   "source": [
    " precision    recall  f1-score  \n",
    "    0.34       0.28     0.27\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864c6a47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
